# Code Supplement to "NL4Py: Agent-Based Modeling in Python with Parallelizable NetLogo Workspaces"

This supplement contains the code used to generate results for "NL4Py: Agent-Based Modeling in Python with Parallelizable NetLogo Workspaces"

## Requirements

The experiments in the article have been conducted on a  Intel(R)  Corei7-7700  CPU  (7th  generation)  PC  with  16GB  of  RAM running  Windows  10  (64Bit),  and  an  AWS  m5.8xlarge  EC2  instance  with  32cores  and  128  GB  memory. Windows 64 Bit or Ubuntu 16.04+ is recommended for the replication of the experiments.


The general requirements to run NL4Py are:
* System must have NetLogo 6 or higher installed. https://ccl.northwestern.edu/netlogo/download.shtml
* System must have JDK 8 or higher installed and set path to java bin included in environment variables. JDK 1.8 is recommended http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html
* System must have Python 2.7 or higher or Python 3.6 or higher installed (Python 3 recommended). Recommend using Anaconda to setup Python. Anaconda will also install common Python packages (Numpy, Pandas, and Matplotlib) that are used in the experiments. https://repo.anaconda.com/archive/Anaconda3-5.2.0-Windows-x86_64.exe


NL4Py has been tested with NetLogo 6.

###Additional Requirements for Experiment 5.2

Please use pip as follows to install the following packages:

```
>pip install nl4py
>pip install deap
>pip install salib
>pip install jpype1
>pip install pynetlogo
>pip install ipyparallel
```
## Instructions

To start the experiments, please use the following command to start the master script:

```python
>python RunAll.py "path_to_netlogo"
```

You will be prompted to enter which experiment you want to run. Please enter one of the following options:
*1: For experiment 3.1, Parameter Calibration with NL4Py and DEAP
*2: For experiment 3.2, Sensitvity Analysis with NL4Py and SALib
*4: For experiment 5.1, Execution time comparisons between NL4Py and PyNetLogo on three different models wihtout parallelism
*5: For experiment 5.2, Execution time comparison between NL4Py schedule_reporters and run_experiment and PyNetLogo run_experiment with multiprocessing

Due to the many replicate runs of simulations and large sample sizes and generations during sensitivity analysis and calibration, full replication of the entire experiment set takes around 48 hours to complete. Therefore, it is recommended to use the master script (RunAlly.py) to run each experiment one at a time.

### Folder structure

*RunAll.py: master script. Please run this file as specified above to initiate replication of results.
*output: Please find all results generated by RunAll.py in this folder.
*models: Contains sample NetLogo models used in the experiments presented in the article.
*Section 3: Contains sensitivity analysis and calibration experiments in Section 3 of the article.
*Section 5: Contains execution time experiments from Section 5 of the article.

### Section 3

In Section 3 we describe two applications of NL4Py important for statistical analysis of agent-based model output, sensitivity analysis and parameter calibration. We perform these methods on the Wolf Sheep Predation model, from the sample models collection in the NetLogo examples library. 

#### Sensitivity Analysis

Sensitivity analysis of the wolf sheep predation model using SALib.

#### Parameter Calibration

Calibraiton of the wolf sheep predation model towards equilibrium using DEAP.

### Section 5

In Section 5 of the article we present results for execution time comparisons of NL4Py under different thread configurations and results for execution time comparisons between NL4Py, PyNetLogo, and PyNetLogo in combination with IPyParallel (as demonstrated by [Jaxa-Rozen & Kwakkel, 2018](http://jasss.soc.surrey.ac.uk/21/2/4.html)).

#### NL4Py vs PyNetLogo Execution Time Comparison

Performs NL4Py vs PyNetLogo simple reporter execution time comparisons on the Fire, Ethnocentrism, and Wolf Sheep Predation models in serial.

#### NL4Py schedule_reporters and run_experiment vs PyNetLogo repeat_report with Multiprocessing

Here we compare the execution time of NL4Py's scheduled reporters and run_experiment against the PyNetLogo equivalent, repeat_report in parallel using multiprocessing library.